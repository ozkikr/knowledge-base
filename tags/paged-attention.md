# paged-attention

*2 notes*

- [[notes/articles/how-prompt-caching-works|How Prompt Caching Works]]
- [[notes/articles/inside-vllm-anatomy-of-a-high-throughput-llm-inference-system|Inside vLLM: Anatomy of a High-Throughput LLM Inference System]]

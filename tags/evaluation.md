# evaluation

*26 notes*

- [[notes/github/agenta|Agenta]]
- [[notes/articles/benchmarking-ai-agent-memory-is-a-filesystem-all-you-need|Benchmarking AI Agent Memory: Is a Filesystem All You Need?]]
- [[notes/github/deepeval|DeepEval]]
- [[notes/github/embedding_explorer|embedding_explorer]]
- [[notes/articles/evaluate-rag-with-llm-evals-and-benchmarking|Evaluate RAG with LLM Evals and Benchmarking]]
- [[notes/articles/evaluating-context-compression-for-ai-agents|Evaluating Context Compression for AI Agents]]
- [[notes/github/hn-time-capsule|hn-time-capsule]]
- [[notes/articles/chessprogramming-king|King - Chessprogramming wiki]]
- [[notes/articles/chessprogramming-king-safety|King Safety - Chessprogramming wiki]]
- [[notes/github/llm-council|LLM Council — Multiple LLMs Answer Together]]
- [[notes/articles/llm-evaluation-practical-tips-at-booking-com|LLM Evaluation: Practical Tips at Booking.com]]
- [[notes/articles/not-so-prompt-prompt-optimization-as-model-selection|Not so Prompt: Prompt Optimization as Model Selection]]
- [[notes/github/onerun|OneRun AI]]
- [[notes/github/opik|Opik]]
- [[notes/github/promptflow|Prompt Flow: End-to-End LLM App Development]]
- [[notes/github/promptfoo|promptfoo — Test Prompts, Agents, and RAGs]]
- [[notes/articles/youtu-agent-quickstart|QuickStart - Youtu-Agent]]
- [[notes/articles/rag-evaluation-quickstart|RAG Evaluation Quickstart]]
- [[notes/github/rag-chunk|rag-chunk]]
- [[notes/github/RAGChecker|RAGChecker]]
- [[notes/github/RagView|RagView]]
- [[notes/github/reactive-agents|reactive-agents]]
- [[notes/articles/tau2-benchmark-improving-results-smaller-models|Tau² Benchmark: How a Prompt Rewrite Boosted GPT-5-mini by 22%]]
- [[notes/articles/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work|Towards a science of scaling agent systems: When and why agent systems work]]
- [[notes/github/web-codegen-scorer|Web Codegen Scorer]]
- [[notes/articles/writing-effective-tools-for-ai-agents|Writing Effective Tools for AI Agents—Using AI Agents]]

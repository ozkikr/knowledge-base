# attention

*2 notes*

- [[notes/articles/streamingllm-attention-sinks|StreamingLLM: How Attention Sinks Keep Language Models Stable]]
- [[notes/articles/visualizing-neural-machine-translation-seq2seq-attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)]]

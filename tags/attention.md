# attention

*4 notes*

- [[notes/articles/nn-attention-and-transformers-bactra|"Attention", "Transformers", in Neural Network "Large Language Models"]]
- [[notes/github/gpt-oss-attention|gpt-oss-attention]]
- [[notes/articles/streamingllm-attention-sinks|StreamingLLM: How Attention Sinks Keep Language Models Stable]]
- [[notes/articles/visualizing-neural-machine-translation-seq2seq-attention|Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)]]

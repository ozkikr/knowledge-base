---
url: https://www.huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language
type: article
tags: [llm-finetuning, pintora, diagrams, lora, unsloth]
date_saved: 2026-02-14
---
# Teaching an LLM a Niche Diagraming Language

## Summary
A walkthrough of fine-tuning Qwen2.5-Coder-7B to learn Pintora, an obscure diagramming language not in its training data. Uses a two-phase approach: continued pretraining (CPT) for syntax learning, then instruction finetuning (IFT) for generation/editing tasks. Training data (~1000-1500 rows) was AI-generated.

## Key Points
- Two-phase training: CPT for syntax, IFT for task-specific abilities
- Used Qwen2.5-Coder-7B as base model with 4-bit quantized LoRA via Unsloth
- Synthetic training data generated by AI due to scarce real-world Pintora code
- Dataset: ~150-200 rows per diagram type (Sequence, ER, Component, Activity, etc.)
- Demonstrates teaching LLMs entirely new domain-specific languages

## Related
- [[llm-finetuning]]
- [[lora]]
- [[code-generation]]

---
url: https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/
type: article
tags: [llm-benchmarks, prompt-engineering, gpt-5-mini, agentic-ai, evaluation]
date_saved: 2026-02-14
---
# Tau² Benchmark: How a Prompt Rewrite Boosted GPT-5-mini by 22%

## Summary
Using the Tau² benchmark framework for evaluating LLM agents, the authors discovered that a simple prompt rewrite improved GPT-5-mini's success rate by over 20% on telecom domain tasks. The article demonstrates how subtle changes to agent policies can unlock significant performance gains in smaller models.

## Key Points
- GPT-5-mini baseline fails ~45% of the time on telecom benchmark tasks
- A prompt rewrite (modifying agent policies/task descriptions) boosted success by 22%
- GPT-5-mini is 2× faster, 5× cheaper than GPT-5 while delivering 85-95% performance
- Tau² simulates real-world agent interactions across telecom, retail, and airline domains
- Demonstrates that prompt engineering can close the gap between small and large models

## Related
- [[llm-evaluation]]
- [[prompt-engineering]]
- [[agentic-ai]]

---
url: https://phoenix.arize.com/evaluate-rag-with-llm-evals-and-benchmarking/
type: article
tags: [rag, evaluation, llm-evals, phoenix, llamaindex]
date_saved: 2026-02-14
---
# Evaluate RAG with LLM Evals and Benchmarking

## Summary
A workshop recap from Arize AI covering how to build a RAG pipeline using LlamaIndex and evaluate it with Phoenix Evals. Walks through the five key RAG stages (loading, indexing, storing, querying, evaluation) with a hands-on code-along exercise.

## Key Points
- Five key RAG stages: Loading, Indexing, Storing, Querying, Evaluation
- Uses Phoenix tracing to capture RAG pipeline data for evaluation
- Builds pipeline with LlamaIndex and evaluates with Phoenix Evals
- Covers vector embeddings and metadata strategies for retrieval accuracy
- Includes Google Colab notebook for hands-on practice

## Related
- [[RAG_Techniques]]
- [[llamaindex]]
- [[deepeval]]

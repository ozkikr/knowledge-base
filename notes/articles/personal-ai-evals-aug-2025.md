---
url: https://darkcoding.net/software/personal-ai-evals-aug-2025/
type: article
tags: [llm-evaluation, model-comparison, personal-evals]
date_saved: 2026-02-14
---
# Evaluating LLMs for My Personal Use Case

## Summary
A developer runs personal evaluations of 11+ LLMs using 130 real prompts from their bash history, covering programming, sysadmin, technical explanations, and general knowledge. Uses blind evaluation methodology with OpenRouter and a custom Rust eval script.

## Key Points
- 130 real prompts from actual usage, categorized into programming, sysadmin, technical, and general
- Models tested: Claude Sonnet 4, DeepSeek v3/R1, Gemini 2.5 Flash/Pro, Kimi K2, GPT-OSS-120B, Qwen3, GLM 4.5
- Blind evaluation methodology to avoid bias
- Uses OpenRouter for all model access
- Qwen3 Coder was "surprisingly bad"; Mercury Coder was decent and fast

## Related
- [[llm-benchmarks]]
- [[openrouter]]
- [[model-selection]]

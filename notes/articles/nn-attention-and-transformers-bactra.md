---
url: https://web.archive.org/web/20250820184917/http://bactra.org/notebooks/nn-attention-and-transformers.html
type: article
tags: [llms, transformers, attention, ml-theory]
date_saved: 2026-02-15
---
# "Attention", "Transformers", in Neural Network "Large Language Models"
## Summary
A long-form technical notebook that interprets attention mechanisms through the lens of classical statistical ideas (especially kernel smoothing), while critically examining terminology and assumptions in modern LLM discourse.

## Key Points
- Frames attention as closely related to Nadarayaâ€“Watson kernel smoothing with learned projections.
- Explains query/key/value mechanics in statistical and linear-algebra terms.
- Questions anthropomorphic naming and hype-driven framing in AI writing.
- Extends discussion from attention to transformer behavior and language modeling claims.
- Emphasizes epistemic caution and iterative understanding over authority.

## Related
- Mechanistic perspectives on transformer internals
- Statistical interpretations of deep learning modules
- Critical reading of AI terminology

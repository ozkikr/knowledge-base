---
url: https://huggingface.co/MiniMaxAI/MiniMax-M2
type: paper
tags: [llm, moe, open-source, coding, agents]
date_saved: 2026-02-14
---
# MiniMax-M2

## Summary
An open-source MoE model (230B total / 10B active parameters) optimized for coding and agentic workflows. Ranks #1 among open-source models on Artificial Analysis benchmarks while being compact and cost-effective to deploy.

## Key Points
- 230B total parameters, only 10B active (MoE architecture)
- Excels at multi-file edits, coding-run-fix loops, and test-validated repairs
- Strong agent performance: plans and executes complex tool chains across shell, browser, retrieval
- Lower latency, lower cost, higher throughput than dense models of similar capability
- #1 open-source composite score on Artificial Analysis benchmarks

## Related
- [[moe]] [[open-source-llm]] [[coding-agents]] [[minimax]]

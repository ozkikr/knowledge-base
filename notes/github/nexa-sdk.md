---
url: https://github.com/NexaAI/nexa-sdk
type: github-repo
tags: [on-device-ai, inference, npu, gpu, cross-platform]
date_saved: 2026-02-14
---
# Nexa SDK

## Summary
NexaSDK is a high-performance local inference framework for running multimodal AI models on-device across NPU, GPU, and CPU. It supports Android, Windows, Linux, macOS, and iOS, with day-0 support for latest models like Qwen3-VL, GPT-OSS, and Granite 4.0.

## Key Points
- Cross-platform: Android, Windows, Linux, macOS, iOS
- Supports NPU, GPU, and CPU inference with hardware acceleration
- Day-0 model support â€” often weeks ahead of competitors (Ollama, llama.cpp)
- Featured by Qualcomm, Google, AMD, NVIDIA, Microsoft, and IBM
- Supports latest models: GPT-OSS, Granite 4.0, Qwen3-VL, Gemma-3n

## Related
- [[on-device-ai]]
- [[local-inference]]
- [[qualcomm-npu]]
- [[mlx-omni-server]]

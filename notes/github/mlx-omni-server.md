---
url: https://github.com/madroidmaq/mlx-omni-server
type: github-repo
tags: [mlx, apple-silicon, local-inference, openai-api, anthropic-api]
date_saved: 2026-02-14
---
# MLX Omni Server

## Summary
MLX Omni Server is a local AI inference server optimized for Apple Silicon (M1-M4), providing dual API compatibility with both OpenAI and Anthropic APIs. It enables seamless local inference using the MLX framework with privacy-first processing.

## Key Points
- Dual API support: OpenAI and Anthropic compatible endpoints
- Complete AI suite: chat, TTS, STT, image generation, embeddings
- Optimized for Apple Silicon M1/M2/M3/M4 chips via MLX framework
- Privacy-first: all processing happens locally
- Drop-in replacement for OpenAI and Anthropic SDKs

## Related
- [[mlx]]
- [[local-inference]]
- [[apple-silicon]]
- [[openai-api-compatible]]

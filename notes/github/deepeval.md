---
url: https://github.com/confident-ai/deepeval
type: github-repo
languages: [Python]
tags: [llm, evaluation, testing, rag, ai-agents]
date_saved: 2026-02-14
---
# DeepEval

## Summary
An open-source LLM evaluation framework similar to Pytest but specialized for unit testing LLM outputs. Supports metrics like G-Eval, hallucination detection, answer relevancy, and task completion using LLM-as-a-judge and local NLP models.

## Key Points
- Pytest-like interface for testing LLM outputs
- Metrics: G-Eval, hallucination, answer relevancy, task completion, and more
- Works with AI agents, RAG pipelines, chatbots (LangChain, OpenAI, etc.)
- Uses LLM-as-a-judge and local NLP models for evaluation
- Companion platform for comparing iterations and generating test reports

## Related
- [[llm-evaluation]] [[testing]] [[rag]] [[ai-agents]] [[pytest]]

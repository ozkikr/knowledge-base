---
url: https://github.com/Michael-A-Kuykendall/shimmy
type: github-repo
languages: [Rust, C, Shell, C++, Python, JavaScript]
tags: [inference-server, openai-compatible, local-llm, rust]
date_saved: 2026-02-15
---
# shimmy
## Summary
shimmy is a Python-free Rust inference server that exposes OpenAI-compatible APIs for local model serving. It focuses on single-binary deployment, model flexibility, and practical drop-in compatibility for existing tooling.
## Key Points
- Runs as a lightweight local inference server with OpenAI-style endpoints.
- Supports GGUF/SafeTensors workflows and model hot-swapping patterns.
- Emphasizes privacy and local execution while preserving API compatibility.
- Designed for simple deployment without Python runtime dependencies.
## Related
- [[Local LLM Serving]]
- [[OpenAI-Compatible APIs]]
- [[Rust AI Infrastructure]]
